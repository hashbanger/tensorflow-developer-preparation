{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.9.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('venv': venv)"
    },
    "interpreter": {
      "hash": "ad093593dd12e9f97c60ccec0f12f02c836472a19186378cedf379d42d354188"
    },
    "colab": {
      "name": "C3_W4_Shakespeare.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3PY95tx-rudj"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_GtOJGFrudY"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDYJaJGvrudc",
        "outputId": "c19fe267-90bf-4636-b5d6-0dfd9a2f250e"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqB-i_FTrudc"
      },
      "source": [
        "## Reading the File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtRoVDUKrudd"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding = 'utf-8')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA8mYkZArude",
        "outputId": "7ac3089c-fa26-4675-ef67-1bd1b4ddbbb8"
      },
      "source": [
        "# length of the text is the number of characters in it\n",
        "print(f\"Length of text: {len(text)} characters\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "408JJI9erude",
        "outputId": "17890e6c-82f4-461c-da04-163ab3f1f566"
      },
      "source": [
        "# Peeking at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnwLMHSArudf",
        "outputId": "753f158c-4e58-4366-c6f8-a054557b6118"
      },
      "source": [
        "# Unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f\"{len(vocab)} unique characters\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQc0fLf_rudf"
      },
      "source": [
        "## Processing the Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuYYZSn1rudf"
      },
      "source": [
        "Before training, you need to convert the strings to a numerical representation.\n",
        "\n",
        "### The preprocessing.\n",
        "`StringLookup` layer can convert each character into a numeric ID.   \n",
        "It just needs the text to be split into tokens first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUYMJI0yrudg",
        "outputId": "51bd2dcd-ee0e-4b64-f60c-b53f2f42265b"
      },
      "source": [
        "example_texts = ['example', 'ghi']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding = 'UTF-8')\n",
        "chars"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'e', b'x', b'a', b'm', b'p', b'l', b'e'], [b'g', b'h', b'i']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhbcRc_9rudg"
      },
      "source": [
        "Creating a preprocessing lookup layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTk8VFMcrudg"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPY4XJBrrudh"
      },
      "source": [
        "It converts from tokens to character IDs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi-ZMuHzrudh",
        "outputId": "5715670a-3e27-4fc1-db7d-2cf582ac809c"
      },
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[44, 63, 40, 52, 55, 51, 44], [46, 47, 48]]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L4z6byJrudh"
      },
      "source": [
        "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it.  \n",
        "For this you can use `preprocessing.StringLookup(..., invert=True)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntUNVjXNrudh"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary = ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJVXdKG2rudi",
        "outputId": "63f5665d-c44b-4606-e2a0-2f9fc587d561"
      },
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'e', b'x', b'a', b'm', b'p', b'l', b'e'], [b'g', b'h', b'i']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv3MzpDdrudi"
      },
      "source": [
        "You can `tf.strings.reduce_join` to join the characters back into strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y4Pm0a_rudi",
        "outputId": "b0b96f6b-016a-4e07-b776-8544cb8f6dfc"
      },
      "source": [
        "tf.strings.reduce_join(chars, axis = -1).numpy()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'example', b'ghi'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH7m3wvurudi"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis = -1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PY95tx-rudj"
      },
      "source": [
        "### The prediction task\n",
        "Given a character, or a sequence of characters, what is the most probable next character?  \n",
        "This is the task you're training the model to perform.  \n",
        "The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz9Amz6erudj"
      },
      "source": [
        "### Create training examples and targets\n",
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.  \n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.  \n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".  \n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41dfU7wQrudj",
        "outputId": "91ebbaa3-049a-450e-f1d9-c480e9798303"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCpnE3lhrudk"
      },
      "source": [
        "ids_dataset=  tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKfPNPpLrudk",
        "outputId": "627ad1ef-1100-4871-bd19-841b1709c1a9"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4iVyJGqrudk"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length + 1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1k4ZjBSrudk"
      },
      "source": [
        "The batch method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMwpilQdrudk",
        "outputId": "4ff9bd6a-efda-4f1c-db99-f9438adcb70d"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder = True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(chars_from_ids(seq))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLUA5h9arudl"
      },
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JgNtbEbrudl",
        "outputId": "448ebeb2-7f75-4e43-cadb-709b76ba4e42"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK37_eyVrudl"
      },
      "source": [
        "For training you'll need a dataset of (input, label) pairs. Where input and label are sequences. At each time step the input is the current character and the label is the next character.  \n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KAHKbAZrudl"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR2S9Ahvrudm",
        "outputId": "af158f98-a4e5-4356-8f32-89a016b9bd95"
      },
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX_n0POtrudm"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL62OisIrudm",
        "outputId": "52c7d084-e72a-4334-ac40-3dcf89fac32a"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input: \", text_from_ids(input_example).numpy())\n",
        "    print(\"Target: \", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target:  b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0lB49aOrudm"
      },
      "source": [
        "### Create training batches\n",
        "You used `tf.data` to split the text into manageable sequences.  \n",
        "But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66LZmz6Trudm",
        "outputId": "9c04ccfd-23de-4678-a0ca-6e2e00a1f0be"
      },
      "source": [
        "# batch size \n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer  in which it shuffles elements.)\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE))\n",
        "dataset"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwBakwX8rudn"
      },
      "source": [
        "### Build The Model\n",
        "This section defines the model as a keras.Model subclass (For details see Making new Layers and Models via subclassing).  \n",
        "\n",
        "This model has three layers:  \n",
        "\n",
        "`tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with embedding_dim dimensions;   \n",
        "`tf.keras.layers.GRU`: A type of RNN with size units=rnn_units (You can also use an LSTM layer here.)  \n",
        "`tf.keras.layers.Dense`: The output layer, with vocab_size outputs. It outputs one logit for each character in the vocabulary.   \n",
        "These are the log-likelihood of each character according to the model.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWmQ3ev0rudn"
      },
      "source": [
        "# length of the vocabulary in characters\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# the embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_a7Ua6Grudn"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                        return_sequences = True,\n",
        "                                        return_state = True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states = None, return_state = False, training = False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training = training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training = training)\n",
        "        x = self.dense(x, training = training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00FDEOu9rudo"
      },
      "source": [
        "model = MyModel(\n",
        "    # the vocabulary size has to match the 'StringLookup' layers.\n",
        "    vocab_size = len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim = embedding_dim, \n",
        "    rnn_units = rnn_units\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srOb4r3trudo"
      },
      "source": [
        "## #Try the model\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPHRYBm9rudo",
        "outputId": "5df8a005-16a0-40e2-e95b-569d43d725a1"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARwwZmU5rudo",
        "outputId": "0ad2649c-28c1-496f-f9fb-6fe9f3409c63"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  16896     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  67650     \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCd8Hpgnrudp"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples = 1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis = -1).numpy()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7cNWrVJrudp",
        "outputId": "91aeefc5-8d2a-43d2-f775-050fa850ddc3"
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([31, 49, 18, 47,  2, 60, 15, 42,  9, 61, 25, 54, 31, 29,  6, 41, 17,\n",
              "       55, 15, 46, 44, 59, 24, 40, 48, 10, 51, 37, 18, 48, 21, 49, 48, 26,\n",
              "        6, 30, 40, 39,  7, 53, 62,  3, 35, 27, 63, 24, 59, 35, 12, 36,  6,\n",
              "       62, 10,  9, 32, 17, 23, 57,  4, 28, 22, 13, 33, 54,  6,  2, 30, 43,\n",
              "       19, 27, 64, 21, 56, 60, 21, 11, 38, 55, 42, 56, 19,  4,  6, 52, 27,\n",
              "       26,  8, 59,  3, 47,  8, 33, 50, 25, 34, 55, 58, 37,  8, 56])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_F0aEwQrudp"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDFUw0vJrudp",
        "outputId": "a4b22679-8881-4289-acae-cb74e7185eb9"
      },
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'thy tongue detect thy base-born heart?\\n\\nEDWARD:\\nA wisp of straw were worth a thousand crowns,\\nTo mak'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"RjEh uBc.vLoRP'bDpBgetKai3lXEiHjiM'QaZ,nw!VNxKtV;W'w3.SDJr$OI?To' QdFNyHquH:YpcqF$'mNM-t!h-TkLUpsX-q\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMYRzjnZrudp"
      },
      "source": [
        "### Train the model\n",
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character.  \n",
        "\n",
        "Attach an optimizer, and a loss function  \n",
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.   \n",
        "\n",
        "Because your model returns logits, you need to set the from_logits flag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSQeUZuNrudq"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atn59jkgrudq",
        "outputId": "9a96bea9-807c-4a41-8b2f-db8018c87306"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.1907787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYYZdkDMrudq"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself,   \n",
        "the output logits should all have similar magnitudes.   \n",
        "To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size.  \n",
        "A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUEHXiJ-rudq",
        "outputId": "b5d3a39d-8907-4cc2-bc17-4d29baa68397"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.07423"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S4igyTtrudq"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use tf.keras.optimizers.Adam with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXEh-eUfrudr"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = loss)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hM7HqWArudr"
      },
      "source": [
        "### Configure checkpoints\n",
        "Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJtqJC-nrudr"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Re-34lSrudr"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulcjuGDtrudr",
        "outputId": "c5d71440-ed61-4c60-ac6d-2b32955184df"
      },
      "source": [
        "history = model.fit(dataset, epochs = EPOCHS, callbacks = [checkpoint_callback])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 26s 131ms/step - loss: 2.7006\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 1.9760\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 1.6998\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 1.5411\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 24s 132ms/step - loss: 1.4434\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 24s 132ms/step - loss: 1.3768\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 24s 132ms/step - loss: 1.3249\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 24s 132ms/step - loss: 1.2801\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 24s 132ms/step - loss: 1.2395\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 24s 132ms/step - loss: 1.2002\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 1.1602\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 24s 132ms/step - loss: 1.1195\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 24s 132ms/step - loss: 1.0762\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 24s 132ms/step - loss: 1.0297\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.9815\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.9313\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 24s 134ms/step - loss: 0.8798\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 24s 132ms/step - loss: 0.8272\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.7764\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 0.7286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAxciUE6wRSa"
      },
      "source": [
        "### Generate text\n",
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.  \n",
        "\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "notMQyRSruds"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        # creating a mask to prevent \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Put a -inf at each bad index.\n",
        "            values = [-float('inf')] * len(skip_ids),\n",
        "            indices = skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape = [len(ids_from_chars.get_vocabulary())]\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states = None):\n",
        "        # Convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        # Run the model.\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                            return_state=True)\n",
        "        # Only use the last prediction.\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits/self.temperature\n",
        "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Sample the output logits to generate token IDs.\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return predicted_chars, states"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXbRd_V2xrE7"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mnnDYNexuqP"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary.   \n",
        "With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtzZUPsHxsoS",
        "outputId": "247c33f7-e6dc-43b6-fd56-99c1602ddcef"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Go to, in heavens! when I commends this,\n",
            "That fresh unwillingned, penitent he speak\n",
            "Main their scars, did ever be found?\n",
            "It shallent do me gracious instal;\n",
            "There shall you hence before he of their deaths.\n",
            "I'll gevorted me by your sides legs desparate;\n",
            "And, if you wish me here behind your brother;\n",
            "And there, thou draw'st, I apply so well:\n",
            "He is nothing to bed what is the rest,\n",
            "Enforce thou use hugh spoke, make her spoke of.\n",
            "\n",
            "First Citizen:\n",
            "Where is thy passago! the name of it!\n",
            "\n",
            "ARIEL:\n",
            "No, as I am, you;--perhaps,\n",
            "Aleay'st Prince Edward, that she speaks:\n",
            "Whither wilt not prove a waked by the day!\n",
            "\n",
            "BALTHASAR:\n",
            "How far off, lords, down forth from me abood\n",
            "That will no more dishinging.\n",
            "\n",
            "AUTOLYCUS:\n",
            "Come, quick from my goods.\n",
            "\n",
            "First Watchman:\n",
            "We are all undonery? O, they had much:\n",
            "And weep shall be dead.\n",
            "\n",
            "SIRININ ELIZABETH:\n",
            "Partly, for he hath warder than the suburbs,\n",
            "Volscians actions patience to-day:\n",
            "That libents that I have to no suborn,\n",
            "I was born to privilege and free\n",
            "And from my heir; my \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.674720048904419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtBL06rFyCKp"
      },
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try EPOCHS = 30).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions.\n",
        "\n",
        "If you want the model to generate text faster the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY_b8c15x07x",
        "outputId": "46f403a1-a7ae-4493-b399-e531fb7f1b95"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nYour mis-shanks you go?\\n\\nMENENIUS:\\nThere was some signeth doth be satisfied.\\n\\nGEORGE:\\nOur general ill!\\nIntended youth-fasliar was?\\n\\nLUCENTIO:\\nI fear, I pray, he ne'er been said,\\nAnd perisheet so much in your cold extreme.\\n\\nQUEEN ELIZABETH:\\nThe sad, he prizes night by the other.\\nO, never fault, and better what this meening?\\nI long to hear them for the place, or happy state,\\nEre he nothing like to his news.\\n\\nLEONTES:\\nNorning, be content.\\n\\nHENRY BOLINGBROKE:\\nEven we dread upon this price, at less banished age\\ntill he indeed,\\nAnd thereto kingly said we parted up, at move\\nMy father gracious in the hand of heaven.\\n\\nProvost:\\nThis is an ass' humbly shall absolute gentlemen:\\nThis did have made thy father's bosom there,\\nThat want at two more sons, which in these unproshes\\nWhat's angry that we are then; but he's more, his ungent.\\n\\nKING RICHARD III:\\nBid my hand I ne'er was married to Mercury?\\nThere is audoriced corn! would buy the\\nbattle; for thy conceit your french\\nAre best before him: some show\"\n",
            " b\"ROMEO:\\nNay, then I see, and I had slept and provent.\\n\\nSLY:\\nWhy, then you please, and let pheers him.\\nBut how may not your friends. Faith, if much behove\\nIn that our foes will bless thee corys unto the ground;\\nNe'er bears him in Verona streets:\\nNor any of our loving friends,' quoth he\\nspeak that, and I tell thee, that he will wedded chole\\nThe brach did sweets that swick himself: but fore-ceased\\nYour judgments in Warwick says\\nYour Rome and Darch by my choices are done.\\n\\nGLOUCESTER:\\nSweet was, Why, Volixenes:\\nCome, I command: thou, hands befees im:\\nBehold this practise as any havens with the\\nAkfully play'd him that thou'rts on thousand, not\\nso leaves, he was the keen, if I may be a place.\\n\\nMENENIUS:\\nWhat's your will?\\n\\nFerand:\\nWhy will you go in anger?\\n\\nDUKE VINCENTIO:\\nPeace, master mayor: then we may clearer by strength\\nMy blood, unwishing her that set themselves;\\nHow had a short bandsome, one thing,\\nTo please you, ven you this crown,\\nWhich enter'd him to hear of severcy,\\nAnd, whilst our gallan\"\n",
            " b\"ROMEO:\\n\\nShepherd:\\nBut, gentle queen, and I bying one sister.\\n\\nPETRUCHIO:\\nNay, I am glad;\\nYou know me, daughter, behold him:\\nI have perform'd, let these things prove attend.\\n\\nKING RICHARD III:\\nI hadeable, and said, the worse, where thou didst brought\\nThe present valours on their bloods,\\nBut little--laceds the presence granted\\nAnd bring their upturation: only that can swear\\nNe'er born enough to do thee good, to disprove untimely Edward's grave,\\nAnd on the duke's report it. To me, brother Claudio;\\nThou hadst dream'st thou untault, thou art a constant coal,\\nBake's heap: for, by the joy of tears,\\nDearsh is my wretched, choler, from trust me;\\nNow yield thy pease, peace, Elbow. You have made\\nGood things rildgum by conclusion,\\nWife we needness twenty thousands.\\nClarence, be thither. Do you not Twilt thou better\\nThan I, beseech you, nirst from truth with the\\nbrotherhood, they could fetch with him\\nTo promise thee never look.\\nI thought no less: it is an hundroy\\nAnd all exavined affairs; bulls, musician\"\n",
            " b\"ROMEO:\\nI should I had entreat your father's death,\\nAnd obsciace more than my love, and le,\\ntheir penty, to diside, a very spotless--\\n\\nBoth Trildner:\\nThou art a wicked powding-dan. Dryour hastes so?\\n\\nBeAM:\\nAnon, if you be mere? fanchions home\\nThe little supposed within her doubled,\\natchiteda's batience; bears this place.\\n\\nPAULINAN:\\nHere, sir; let him\\nWarwick rain and cold, possess.\\n\\nBISHOP OF ELY:\\nThy censualed lady.\\n\\nMENENIUS:\\nHa! he will love and embrace, sir, be gone;\\nThere's water rather with our country's love\\nAntiated's death, if thy benefith thou\\nplain,--O, I am sure, not thither.\\nWhat's orn? when whispering? Mars!\\n\\nELBOW:\\nMarry, I thank you, rather: pray you, sir.\\n\\nFLORIZEL:\\nWhere it deliver them: what sands\\nHe cannot well dry I should bid ghoself\\nThan first as fin to ask, in firm edgening,\\nMay wind and Richard, Like George's prince,s come.\\nFor this, because the day one, Myself,\\nOr that, as these flaverse nation, hath been:\\nNow in this masks cloud by rage and roar'd,\\nAs he from that n\"\n",
            " b\"ROMEO:\\nThis delights did cast upon thy father's house,\\nIf entrance our games: if but as much followers.\\n\\nYORK:\\nI shall jo under us here trad forth that banish'd foil-grave,\\nAnd that dead manifest lights Somerset,\\nBid thee this out-dared bastards,\\nAnd every thing in extremity of,\\nHere city clied and princely Rudloness.\\n\\nDORCAS:\\nWe must suffer not their dam: but he was ready\\nTo wake one here.\\n\\nShepherd:\\nWill you tender year ol Warwick on kissing?\\nOr is the clouds that bids begin to speak.\\n\\nWARWICK:\\nAnd, true! thy poor issue of thy words,\\nThat Jesh our lancements that will play,\\nDost lack for Rambany.\\n\\nMENENIUS:\\nWhy, what, I'll believe that, I and thus\\nWhiles other most beat off the shrew doth great;\\nTo show thee, on thy pains there, had thy daughter,\\nYou far as like that passa life's blow,\\nAs falcon, 'love, 'twixt Gull's air,\\nAnd yet my country I have dispatch'd with wings\\nHad holp your triumphant lords, never\\nWas I took unto his hands\\nThe which you are of unlawful bed, and darks he make\\nthee \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.250874757766724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frv5Hqu5yVAx"
      },
      "source": [
        "The full tutorial https://www.tensorflow.org/text/tutorials/text_generation"
      ]
    }
  ]
}